perceptron and:

import numpy as np

# Step-1: Initialize Input and Output Variables
features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input combinations
labels = np.array([0, 0, 0, 1])  # AND Gate Output

# Step-2: Initialize Network Parameters
learning_rate = float(input("Enter the learning rate: "))
threshold = float(input("Enter the threshold: "))

# Initialize weights and bias
w = np.array([float(input("Enter weight for x0: ")), float(input("Enter weight for x1: "))])
bias = float(input("Enter bias: "))

epoch = 0  # Counter for iterations

# Training Loop (Run until output == target for all inputs)
while True:
    global_error = 0
    print(f"\nEpoch {epoch + 1}:")
    
    for i in range(features.shape[0]):
        x = features[i]  # Current input
        actual = labels[i]  # Target output
        
        # Compute weighted sum
        sum_unit = np.dot(x, w) + bias
        
        # Activation function (Step function)
        fire = 1 if sum_unit > threshold else 0
        
        # Compute error
        error = actual - fire
        global_error += abs(error)
        
        # Update weights & bias if there's an error
        if error != 0:
            w += learning_rate * error * x
            bias += learning_rate * error

        print(f"Input: {x}, Prediction: {fire}, Target: {actual}, Error: {error}")
        print(f"Updated Weights: {w}, Updated Bias: {bias}\n")

    epoch += 1  # Increment epoch counter
    
    # Stop training when all predictions match targets
    if global_error == 0:
        print("\nðŸŽ‰ Training converged! No further updates required.")
        break

# Final Weights & Bias
print("\nFinal Weights:", w)
print("Final Bias:", bias)
print("Total Epochs:", epoch)



Enter the learning rate: 0.4
Enter the threshold: 0.3
Enter weight for x0: 0.4
Enter weight for x1: 0.6
Enter bias: 0.2

Epoch 1:
Input: [0 0], Prediction: 1, Target: 0, Error: -1
Updated Weights: [0.4 0.6], Updated Bias: -0.2

Input: [0 1], Prediction: 1, Target: 0, Error: -1
Updated Weights: [0.4 0.2], Updated Bias: -0.6

Input: [1 0], Prediction: 1, Target: 0, Error: -1
Updated Weights: [0.  0.2], Updated Bias: -1.0

Input: [1 1], Prediction: 0, Target: 1, Error: 1
Updated Weights: [0.4 0.6], Updated Bias: -0.6

Epoch 2:
Input: [0 0], Prediction: 0, Target: 0, Error: 0
Input: [0 1], Prediction: 0, Target: 0, Error: 0
Input: [1 0], Prediction: 0, Target: 0, Error: 0
Input: [1 1], Prediction: 1, Target: 1, Error: 0

ðŸŽ‰ Training converged! No further updates required.

Final Weights: [0.4 0.6]
Final Bias: -0.6
Total Epochs: 2


#naive

# importing the libraries
import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd
import seaborn as sns
# importing the dataset
dataset = pd.read_csv('NaiveBayes.csv')
# Seperating Independent and Dependent Variable
x = dataset.iloc[:, [0,1]].values 
y = dataset.iloc[:, 2].values 
print(x)
Output:
array([[ 19, 19000],
[ 35, 20000],
[ 26, 43000],
[ 27, 57000],
[ 19, 76000],
[ 27, 58000],
[ 47, 23000],
[ 49, 36000]], dtype=int64)

print(y)
Output:
array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
1, 1, 0, 1], dtype=int64)
# training and testing data
from sklearn.model_selection import train_test_split
# assign test data size 25%
X_train, X_test, y_train, y_test =train_test_split(X,y,test_size= 0.25, random_state=0)
# import Gaussian Naive Bayes classifier 
from sklearn.naive_bayes import GaussianNB 
# create a Gaussian Classifier
classifer1 = GaussianNB()
# training the model 
classifer1.fit(X_train, y_train) 
# testing the model
y_pred1 = classifer1.predict(X_test) 
print(y_pred1)

Output:
array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1], dtype=int64)
# importing accuracy score
from sklearn.metrics import accuracy_score 
# printing the accuracy of the model 
print(accuracy_score(y_test,y_pred1)) 
Output:
0.9
# importing the required modules
import seaborn as sns
from sklearn.metrics import confusion_matrix
# passing actual and predicted values
cm = confusion_matrix(y_test, y_pred1)
# true write data values in each cell of the matrix 
sns.heatmap(cm,annot=True) 
plt.savefig('confusion.png')
Output:

# importing classification report
from sklearn.metrics import classification_report 
# printing the report 
print(classification_report(y_test, y_pred1)) 
Output:
precision recall f1-score support
0 0.90 0.96 0.93 68
1 0.89 0.78 0.83 32
accuracy 0.90 100
macro avg 0.90 0.87 0.88 100
weighted avg 0.90 0.90 0.90 100


